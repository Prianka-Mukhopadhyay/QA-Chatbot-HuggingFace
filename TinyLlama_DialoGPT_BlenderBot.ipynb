{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOVTqUkRCTrj1X4c8Js36dP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Prianka-Mukhopadhyay/QA-Chatbot-HuggingFace/blob/main/TinyLlama_DialoGPT_BlenderBot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bfi3hz5pfTgV"
      },
      "outputs": [],
      "source": [
        "# Install Hugging Face libraries\n",
        "!pip install -q transformers accelerate\n",
        "\n",
        "# Import required libraries\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "\n",
        "# Load TinyLlama (very lightweight, good for CPU)\n",
        "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"cpu\",   # Force CPU\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using TinyLlama"
      ],
      "metadata": {
        "id": "J5An4CitkbKj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a chatbot pipeline\n",
        "chatbot = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    #device=-1   # CPU\n",
        ")\n",
        "\n",
        "# Function to chat with the model\n",
        "def ask_bot(prompt, max_new_tokens=200):\n",
        "    response = chatbot(\n",
        "        prompt,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9\n",
        "    )\n",
        "    return response[0]['generated_text']\n",
        "\n",
        "# Test the chatbot\n",
        "context = \"Hugging Face is a company that develops tools for natural language processing.\"\n",
        "question = \"What does Hugging Face specialize in?\"\n",
        "\n",
        "prompt = f\"Context: {context}\\n\\nQuestion: {question}\\nAnswer:\"\n",
        "print(ask_bot(prompt))\n"
      ],
      "metadata": {
        "id": "_NpYd-zNg-cf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tests = [\n",
        "    {\n",
        "        \"context\": \"The Eiffel Tower is located in Paris, France.\",\n",
        "        \"question\": \"Where is the Eiffel Tower located?\"\n",
        "    },\n",
        "    {\n",
        "        \"context\": \"Python is a programming language often used for machine learning.\",\n",
        "        \"question\": \"What is Python commonly used for?\"\n",
        "    },\n",
        "    {\n",
        "        \"context\": \"Cristiano Ronaldo is a professional football player from Portugal.\",\n",
        "        \"question\": \"What sport does Cristiano Ronaldo play?\"\n",
        "    }\n",
        "]\n",
        "\n",
        "for t in tests:\n",
        "    prompt = f\"Context: {t['context']}\\n\\nQuestion: {t['question']}\\nAnswer:\"\n",
        "    print(\"Q:\", t['question'])\n",
        "    print(\"A:\", ask_bot(prompt))\n",
        "    print(\"-\" * 50)\n"
      ],
      "metadata": {
        "id": "-G1j8cBrhq_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using DialoGPT-small"
      ],
      "metadata": {
        "id": "6TkgCJL9kpbj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "# Load DialoGPT-small\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-small\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-small\")\n",
        "\n",
        "# Start chatting\n",
        "chat_history_ids = None\n",
        "print(\"Chatbot is ready! Type 'quit' to stop.\\n\")\n",
        "\n",
        "for step in range(5):  # 5 turns of dialogue\n",
        "    user_input = input(\"You: \")\n",
        "    if user_input.lower() == \"quit\":\n",
        "        break\n",
        "\n",
        "    # Encode user input & append to chat history\n",
        "    new_input_ids = tokenizer.encode(user_input + tokenizer.eos_token, return_tensors='pt')\n",
        "    bot_input_ids = torch.cat([chat_history_ids, new_input_ids], dim=-1) if chat_history_ids is not None else new_input_ids\n",
        "\n",
        "    # Generate response\n",
        "    chat_history_ids = model.generate(\n",
        "        bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    # Decode & print\n",
        "    bot_reply = tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)\n",
        "    print(f\"Bot: {bot_reply}\\n\")\n"
      ],
      "metadata": {
        "id": "JRgwRWG_iQqG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##** BlenderBot Small**"
      ],
      "metadata": {
        "id": "YF1flZjooMXT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers gradio -q\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import gradio as gr\n",
        "\n",
        "MODEL_ID = \"facebook/blenderbot_small-90M\"  # light, dialogue-tuned\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_ID)"
      ],
      "metadata": {
        "id": "wwEqpviGkU83"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chat_fn(message, history):\n",
        "    # Build a short context from the last few turns to keep RAM low\n",
        "    past = \"\\n\".join([f\"User: {u}\\nBot: {b}\" for u,b in history[-3:]])  # last 3 turns\n",
        "    prompt = (past + \"\\n\" if past else \"\") + f\"User: {message}\\nBot:\"\n",
        "    inputs = tokenizer(\n",
        "        [prompt],\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        max_length=256,          # keep input small on CPU\n",
        "        padding=False,\n",
        "    )\n",
        "    output_ids = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=120,      # cap output length\n",
        "        do_sample=True,\n",
        "        top_p=0.9,\n",
        "        temperature=0.8,\n",
        "        no_repeat_ngram_size=3,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "    reply = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0]\n",
        "    # If the model echoed the prompt, grab only the part after \"Bot:\"\n",
        "    reply = reply.split(\"Bot:\")[-1].strip() if \"Bot:\" in reply else reply.strip()\n",
        "    return reply\n",
        "\n",
        "demo = gr.ChatInterface(\n",
        "    fn=chat_fn,\n",
        "    title=\"BlenderBot Small (CPU)\",\n",
        "    description=\"Lightweight conversational bot that runs on CPU.\",\n",
        "    examples=[\"hey\", \"what are you doing?\", \"tell me a joke\"],\n",
        ")\n",
        "demo.launch()"
      ],
      "metadata": {
        "id": "PYGpx0KtnpVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chat function\n",
        "def chat(history, user_input):\n",
        "    inputs = tokenizer(user_input, return_tensors=\"pt\")\n",
        "    reply_ids = model.generate(**inputs)\n",
        "    reply = tokenizer.decode(reply_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    # Append to chat history\n",
        "    history = history + [(user_input, reply)]\n",
        "    return history, history\n",
        "\n",
        "# Gradio UI\n",
        "with gr.Blocks() as demo:\n",
        "    chatbot = gr.Chatbot()\n",
        "    state = gr.State([])  # keeps track of conversation history\n",
        "    msg = gr.Textbox(label=\"Type your message here...\")\n",
        "\n",
        "    def respond(user_message, history):\n",
        "        history, updated_history = chat(history, user_message)\n",
        "        return \"\", updated_history\n",
        "\n",
        "    msg.submit(respond, [msg, state], [msg, chatbot])\n",
        "\n",
        "demo.launch(debug='True')\n"
      ],
      "metadata": {
        "id": "3j5iqWkslOLZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}